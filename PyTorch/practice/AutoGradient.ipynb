{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Automatic Gradient Calculation",
   "id": "3227fe98fe7e2a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To train a neural network, we need to use the algorithm called back propagation. This alogirhtm requires the gradient of the loss function with respect to the parameters of the model. In this notebook, we will see how to calculate the gradient of a function using automatic differentiation.\n",
    "\n",
    "PyTorch provides a built-in module, `torch.autograd`, for automatic differentiation. We can use it to automatically calculate the gradients of the loss function with respect to the parameters."
   ],
   "id": "115a2599e4cfff0c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-25T03:03:07.860382Z",
     "start_time": "2024-08-25T03:03:06.211116Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tensors, Functions and Computational graph",
   "id": "9768d9d0e8cdedad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![](https://pytorch.org/tutorials/_static/img/basics/comp-graph.png)\n",
    "\n",
    "`w` and `b` are the parameters of the model that we want to optimize. We should be able to compute the gradients of the loss function with respect to these variables. To do this, we set the `requires_grad` property of the tensors. \n"
   ],
   "id": "8ac5e7dbe35353c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T03:16:19.925352Z",
     "start_time": "2024-08-25T03:16:19.921125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ],
   "id": "1a89512cd72771a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000001F650ADA350>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000001F650ADA4A0>\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The functions we apply to tensors are an object of class `Function`. The object knows how to compute the function in the forward direction and its derivative during the backward pass. A reference to the backward propagation. The reference to backward propagation function is stored in the `grad_fn` property of a tensor.",
   "id": "5fa23e6cef729237"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Computing Gradients",
   "id": "18610c873cfd207b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$\\frac{\\partial loss}{\\partial w}$ and $\\frac{\\partial loss}{\\partial b}$ can be computed by calling `loss.backward()`. The value of the gradients are stored in the `.grad` property of the respective tensors.",
   "id": "30eec97db681cc31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T10:36:11.810752Z",
     "start_time": "2024-08-25T10:36:11.747955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ],
   "id": "fc5b55025df85364",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0023, 0.0586, 0.3229],\n",
      "        [0.0023, 0.0586, 0.3229],\n",
      "        [0.0023, 0.0586, 0.3229],\n",
      "        [0.0023, 0.0586, 0.3229],\n",
      "        [0.0023, 0.0586, 0.3229]])\n",
      "tensor([0.0023, 0.0586, 0.3229])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Disabling Gradient Tracking",
   "id": "f7e1f5a2ba8f0d76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "All tensors have a default of `requires_grad=True`. \n",
    "\n",
    "For only forward computational tensors, we can set the `requires_grad` property to `False`. This can be done using the `torch.no_grad()` context manager.\n",
    "Or, we can also use the `detach()` method on the tensor to create a new tensor that does not require gradients."
   ],
   "id": "6f95fa113c655619"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T10:39:38.416530Z",
     "start_time": "2024-08-25T10:39:38.412674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "z=torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ],
   "id": "ab4ef08579bb4f5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T10:40:11.380858Z",
     "start_time": "2024-08-25T10:40:11.375824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ],
   "id": "78359b6ebfea8a72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Those techniques are useful for the following situations:\n",
    "- To make some parameters in the neural network to be frozen.\n",
    "- To speed up computations when we are only doing forward pass."
   ],
   "id": "4b7db8608b29919a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## More on Computational Graphs",
   "id": "efe9156c750b18e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Auto gradient keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG). This is called a computational graph.\n",
    "\n",
    "Forward Pass\n",
    "- run the input tensor through the model\n",
    "- maintain a record of the operations in the DAG\n",
    "\n",
    "Backward Pass\n",
    "- compute the gradients of the loss function with respect to the parameters using back propagation\n",
    "- the gradients are computed using the chain rule"
   ],
   "id": "d57dd120365b8955"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tensor Gradients and Jacobian Products",
   "id": "b687b449321a087a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "PyTorch computes the *Jacobian product* rather than the Jacobian matrix itself.\n",
    "\n",
    "Vector function $\\vec{y}=f(\\vec{x})$ where $\\vec{x}=<x_1, x_2, ... x_n>$ and $\\vec{y}=<y_1, y_2, ... y_m>$.\n",
    "\n",
    "The Jacobian matrix is given by:\n",
    "$$\n",
    "J=\\begin{pmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "When $v$ is the gradient tensor of the scalar loss function with respect to $\\vec{y}$.\n",
    "$$\n",
    "v=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial loss}{\\partial y_1} \\\\\n",
    "\\frac{\\partial loss}{\\partial y_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial loss}{\\partial y_m}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The **Jacobian product** is $v^T \\cdot J$. \n",
    "$$\n",
    "v^T \\cdot J=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial loss}{\\partial y_1} & \\frac{\\partial loss}{\\partial y_2} & \\cdots & \\frac{\\partial loss}{\\partial y_m}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial loss}{\\partial x_1} \\\\\n",
    "\\frac{\\partial loss}{\\partial x_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial loss}{\\partial x_n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "This product can be computed using the `backward` method of the tensor."
   ],
   "id": "7f3ff73a98ae8101"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T12:07:25.481437Z",
     "start_time": "2024-08-25T12:07:23.458682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2).T\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(\"First call\\n\", inp.grad)\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(\"\\nSecond call\\n\", inp.grad)\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
   ],
   "id": "de45011cb657a8ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "\n",
      "Second call\n",
      " tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      " tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
